{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0bd9U0ktaVA0oEvQms/4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiX7000/deep-learning-with-python/blob/main/DeepLearning_with_Python_Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning examples from the book 'Deep Learning with Python', Part 4, Francois Chollet"
      ],
      "metadata": {
        "id": "go0BrbHW5AR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1. Functional API"
      ],
      "metadata": {
        "id": "wDN1ppmY5FWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we need to have several independent inputs, multiple outputs or more complex internal topology/processes, like inception modules and residual connections(from page 242), functional API helps us to manipulate layers as functions taking tensors as inputs and outputs. In addition, we can reuse a layer instance several times(meaning we can share the same weights/knowledge without creating a new layer) and also use whole models as layers(@page247)"
      ],
      "metadata": {
        "id": "ckvHhxD_7YYi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest functional API model with 1 input and 1 output"
      ],
      "metadata": {
        "id": "z7rpXuZl8h23"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbfOP6yk4s6v"
      },
      "outputs": [],
      "source": [
        "# define a functional API model\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "input_tensor = Input(shape=(64,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "x = layers.Dense(32, activation='relu')(x)\n",
        "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(input_tensor, output_tensor)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile, train and evaluate the functional API model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "# create some very simple data\n",
        "import numpy as np\n",
        "x_train = np.random.random((1000, 64))\n",
        "y_train = np.random.random((1000, 10))\n",
        "\n",
        "# train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
        "\n",
        "# evaluate the model\n",
        "score = model.evaluate(x_train, y_train)\n",
        "print('\\nThe score of the model is:', score)"
      ],
      "metadata": {
        "id": "OocxSup14vwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-input functional API model(question-answering model) with 1 output"
      ],
      "metadata": {
        "id": "6FV4ik95-2OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a functional API model with 2 inputs: a reference text and a question\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "text_vocabulary_size = 10000\n",
        "question_vocabulary_size = 10000\n",
        "answer_vocabulary_size = 500\n",
        "\n",
        "# define 'text' part of NN\n",
        "\n",
        "# input of text part of NN\n",
        "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
        "# 1st layer of text part of NN\n",
        "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)\n",
        "# 2nd layer of text part of NN\n",
        "encoded_text = layers.LSTM(32)(embedded_text)\n",
        "\n",
        "# define 'question' part of NN\n",
        "\n",
        "# 1st layer of question part of NN\n",
        "question_input = Input(shape=(None,), dtype='int32', name='question')\n",
        "# 1st layer of question part of NN\n",
        "embedded_question = layers.Embedding(32, question_vocabulary_size)(question_input)\n",
        "# 2nd layer of question part of NN\n",
        "encoded_question = layers.LSTM(16)(embedded_question)\n",
        "\n",
        "# concatenate the above 2 parts \n",
        "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
        "\n",
        "# put the above into a final dense layer\n",
        "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
        "\n",
        "model = Model([text_input, question_input], answer)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ntY876Px4vt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model like this, we can feed the model either a list of numpy arrays as inputs or a dictionary that maps input names to numpy arrays"
      ],
      "metadata": {
        "id": "6f6n5D-3CSZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile and train the functional API model\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# create some random data\n",
        "import numpy as np\n",
        "num_samples = 1000\n",
        "max_length = 100\n",
        "\n",
        "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))   # generates dummy numpy data\n",
        "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
        "answers = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size))   # answers are one-hot encoded, not integers\n",
        "\n",
        "# train the model\n",
        "model.fit([text, question], answers, epochs=10, batch_size=128) # fitting using a list of inputs\n",
        "\n",
        "# fitting using a dictionary of inputs(only if inputs are named)\n",
        "#model.fit( {'text':text, 'question':question}, answers, epochs=10, batch_size=128 )"
      ],
      "metadata": {
        "id": "HjbC0inq4vrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-output functional API model with 1 input"
      ],
      "metadata": {
        "id": "uLpjIRPsEq68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model takes as input a series of posts of social media from a single person and tries to predict attributes of that person, such as age, gender and income level"
      ],
      "metadata": {
        "id": "H7KoAO84E3Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "vocabulary_size = 50000\n",
        "num_income_groups = 10\n",
        "\n",
        "# model's architecture\n",
        "posts_inputs = Input(shape=(None,), dtype='int32', name='posts')\n",
        "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_inputs)\n",
        "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "x = layers.MaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "# model's 3 outputs defining\n",
        "age_prediction = layers.Dense(1, name='age')(x)\n",
        "income_prediction = layers.Dense(num_income_groups, activation='softmax', name='income')(x)\n",
        "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
        "\n",
        "model = Model(posts_inputs, [age_prediction, income_prediction, gender_prediction])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sYGMRuRK4voZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3354e0cb-90d6-4725-eb41-6fa7bd1aadb8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " posts (InputLayer)             [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, None, 50000)  12800000    ['posts[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, None, 128)    32000128    ['embedding_8[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, None, 128)    0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, None, 256)    164096      ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, None, 256)    327936      ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, None, 256)   0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, None, 256)    327936      ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, None, 256)    327936      ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, None, 256)   0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, None, 128)    32896       ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " age (Dense)                    (None, None, 1)      129         ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " income (Dense)                 (None, None, 10)     1290        ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " gender (Dense)                 (None, None, 1)      129         ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 45,982,476\n",
            "Trainable params: 45,982,476\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have multiple outputs, training requires to specify different loss functions for different heads of network. The simplest way to combine differet losses is to sum them all"
      ],
      "metadata": {
        "id": "yazuxe__HzLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "#model.compile(optimizer='rmsprop', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])\n",
        "\n",
        "# if you have given names to output layers\n",
        "#model.compile(optimizer='rmsprop', loss={'age':'mse', 'income':'categorical_crossentropy', 'gender':'binary_crossentropy'})\n",
        "\n",
        "# we assign different levels of importance to the losses\n",
        "model.compile(optimizer='rmsprop', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
        "              loss_weights=[0.25, 1., 10.])"
      ],
      "metadata": {
        "id": "XC4UZ8Dk4vlp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model.fit(posts_inputs, [age_prediction, income_prediction, gender_prediction], epochs=10, batch_size=64) # age_targets,income_targets and gender_targets are numpy arrays!\n",
        "\n",
        "# and if you have given names to output layers\n",
        "#model.fit(posts, {'age':age_targets, 'income':income_targets, 'gender':gender_targets}, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "id": "IU0qG9kx4vig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqoP8UuMTIsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-input, Multi-output functional API model"
      ],
      "metadata": {
        "id": "tyOMxlWmBQg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "import keras\n",
        "\n",
        "vocabulary_size = 10000\n",
        "num_tags = 100\n",
        "num_departments = 4\n",
        "\n",
        "# model's architecture\n",
        "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
        "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
        "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
        "\n",
        "features = layers.Concatenate()([title, text_body, tags])\n",
        "features = layers.Dense(64, activation=\"relu\")(features)\n",
        "\n",
        "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
        "department = layers.Dense(\n",
        "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
        "\n",
        "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
      ],
      "metadata": {
        "id": "-yTV3N7WBWoH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "num_samples = 1280\n",
        "\n",
        "# the data\n",
        "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
        "\n",
        "priority_data = np.random.random(size=(num_samples, 1))\n",
        "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
        "\n",
        "# compile, train, evaluate and predict\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
        "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
        "model.fit([title_data, text_body_data, tags_data],\n",
        "          [priority_data, department_data],\n",
        "          epochs=1)\n",
        "model.evaluate([title_data, text_body_data, tags_data],\n",
        "               [priority_data, department_data])\n",
        "priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])"
      ],
      "metadata": {
        "id": "7y2VzH7bBmM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xfs4R1cICEBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2. Using callbacks and tensorboard"
      ],
      "metadata": {
        "id": "x4o0zDi4Ti5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callbacks: a way to measure that the validation loss is no longer improving, so we can stop training and avoid wasting time. They can interrupt training, save a model, load a different weight set or alter the state of the model. Some ways you can use callbacks: model checkpointing, early stopping, dynamically adjusting the value of certain parameters during training, logging training and validation metrics during training or visualizing the representations learned by the model as they are updated. Some callbacks that keras module includes:\n",
        "*   keras.callbacks.ModelCheckpoint\n",
        "*   keras.callbacks.EarlyStopping\n",
        "*   keras.callbacks.LearningRateScheduler\n",
        "*   keras.callbacks.ReduceLROnPlateau\n",
        "*   keras.callbacks.CSVLogger\n",
        "\n"
      ],
      "metadata": {
        "id": "PFB6J-YdTrSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how to use ModelCheckpoint and EarlyStopping\n",
        "import keras\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.EarlyStopping(monitor='acc', patience=1,),    # interrupts when improvement stops=> when validation accuracy has stopped improving for more than 1 epoch\n",
        "    keras.callbacks.ModelCheckpoint(filepath='mymodel.h5', monitor='val_loss', save_best_only=True,)    # saves the weights after every epoch, if and only if validation loss has improved, which allows us to keep the best model\n",
        "]\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "model.fit(x, y, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "BXru92aTToAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also write your own callback(@page251)"
      ],
      "metadata": {
        "id": "tDJh1toyXjQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorBoard: a browser-based useful tool that helps us visually monitor everything that goes inside your model during training. In this way, you develop a clearer vision of what your model does and does not do. Specifically, you can: 1) visually monitor metrics during training, 2) visualize your model's architecture, 3) visualize histograms of activations and gradients, 4) explore embeddings in 3D"
      ],
      "metadata": {
        "id": "WfwCn7kMXrlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see an example of how to use tensorboard in a text classification model implementation"
      ],
      "metadata": {
        "id": "Nv1N13_YYu-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Sequential\n",
        "from keras.datasets import imdb\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "max_features = 2000\n",
        "maxlen = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)   # loads data as lists of integers\n",
        "\n",
        "# all reviews with max_len words\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# model's architecture\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=maxlen, name='embed'))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))  \n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "a6kovXq_U3H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to start using tensorboard, we need to create a directory where we will store the log files it generates\n",
        "!mkdir my_log_dir"
      ],
      "metadata": {
        "id": "UgNKwtdCabd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start training with a tensorboard callback\n",
        "callbacks = [ keras.callbacks.TensorBoard(log_dir='full path to my_log_dir', histogram_freq=1, embeddings_freq=1,) ] # records activation histograms and embedding data every 1 epoch\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.2, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "vRhwHnzOas4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /full_path_to my_log_dir"
      ],
      "metadata": {
        "id": "IKo0jlw6CumD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SLNoNEk4b9rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir celeba_gan\n",
        "!wget https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n",
        "!unzip -qq celeba_gan/data.zip -d celeba_gan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SydHdEKCDm-I",
        "outputId": "8f8d15d9-d639-4eef-e3f6-51f480e5399c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘celeba_gan’: File exists\n",
            "--2022-11-30 19:51:53--  https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.213.139, 173.194.213.102, 173.194.213.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.213.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=drive_open [following]\n",
            "--2022-11-30 19:51:53--  https://drive.google.com/file/d/0B7EVK8r0v71pZjFTYXZWM3FlRnM/view?usp=drive_open\n",
            "Reusing existing connection to drive.google.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM’\n",
            "\n",
            "open?id=0B7EVK8r0v7     [ <=>                ]  72.92K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-11-30 19:51:55 (21.4 MB/s) - ‘open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM’ saved [74667]\n",
            "\n",
            "unzip:  cannot find or open celeba_gan/data.zip, celeba_gan/data.zip.zip or celeba_gan/data.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ux40oJPEDm60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbMPRZ91Dm0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some general tips to advance your models and Bye!"
      ],
      "metadata": {
        "id": "CoL-cRi_c3-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When building high-performing deep convnets, try using BatchNormalization layer or SeparableConv2D layer(@page260)"
      ],
      "metadata": {
        "id": "jnGmCqddc8Ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter optimization:\n",
        "*   choose a set of hyperparameters(automatically)\n",
        "*   build the corresponding model\n",
        "*   fit it to your training data, and measure the final performance on validation data\n",
        "*   choose the next set of hyperparameters to try(automatically)\n",
        "*   repeat\n",
        "*   when no improvement, choose the best model and measure performance on your test data\n",
        "\n"
      ],
      "metadata": {
        "id": "wfRlTczadYsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several techniques to automatically choose hyperparameters like Bayesian optimization, genetic algorithms, simple random search, and so on. Try Hyperopt(https://github.com/hyperopt/hyperopt)"
      ],
      "metadata": {
        "id": "dlowu_0leUAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another very powerful technique to improve performance of your model is model ensembling: consists of pooling together the predictions of a set of models. The simplest ensembling is to give the same importance/weights to all different model predictions "
      ],
      "metadata": {
        "id": "cqPYrZUWe9KL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last chapter of the book consists of some advanced concepts like text generation with LSTM, DeamDream, my favorite Neural style transfer, generating images with variational autoencoders and GANs. I won't make any further reference to all of them at this repo at this time"
      ],
      "metadata": {
        "id": "y5v6aiksgSW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also find the official implementations of all the examples I presented in all parts of this repo(which are also included to the book) here: https://github.com/fchollet/deep-learning-with-python-notebooks"
      ],
      "metadata": {
        "id": "01JQsERBhKn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it from me, this was a quick introduction to deep learning with Python. The next step is to experiment with many different datasets and models for several tasks. I wish you GOOD LUCK !!"
      ],
      "metadata": {
        "id": "xh0vaSsifu2V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5JLbVEdc7hD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}